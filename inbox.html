<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>VoiceMate Inbox</title>
  <style>
    body {
      font-family: 'Segoe UI', sans-serif;
      background-color: #0c0f13;
      color: white;
      text-align: center;
      padding: 2rem;
    }

    #orb {
      width: 100px;
      height: 100px;
      background-color: red;
      border-radius: 50%;
      margin: 1rem auto;
      cursor: pointer;
      box-shadow: 0 0 10px rgba(255, 0, 0, 0.7);
    }

    #orb.listening {
      animation: pulse 1.2s infinite;
    }

    @keyframes pulse {
      0% { transform: scale(1); }
      50% { transform: scale(1.1); }
      100% { transform: scale(1); }
    }

    #status, #timer {
      margin-top: 0.5rem;
      font-size: 1rem;
      color: #ccc;
    }

    textarea {
      width: 90%;
      max-width: 500px;
      height: 100px;
      margin: 1rem auto;
      display: block;
      font-size: 1rem;
      padding: 1rem;
      border-radius: 10px;
      border: none;
    }

    audio {
      display: block;
      margin: 1rem auto;
    }
    .cta-button {
  padding: 0.5rem 1rem;
  border-radius: 20px;
  background: #FA4B53;
  color: white;
  border: none;
  cursor: pointer;
  font-size: 1rem;
}


    button {
      padding: 1rem 2rem;
      background-color: #fa4b53;
      color: white;
      border: none;
      font-size: 1rem;
      border-radius: 8px;
      cursor: pointer;
      margin-top: 1rem;
    }

    p.hint {
      font-size: 0.9rem;
      color: #aaa;
    }
  </style>
</head>
<body>
    <header style="display: flex; align-items: center; justify-content: center; padding: 1rem 0;">
  <img src="assets/img/VoiceMate_logo_horiz_dark.png" alt="VoiceMate Logo" style="height: 48px;" />
</header>

  <h2>üé§ Tap the Orb to Start / Stop Recording</h2>
  <p class="hint">Then press Send Pulse when ready.</p>
  <div id="orb" title="Tap to Record"></div>
  <div id="status">Idle</div>
  <div id="timer">00:00</div>

  <textarea id="transcript" placeholder="Your transcript will appear here..."></textarea>
  <audio id="audio" controls></audio>
  <!-- Add this to the inbox.html where you want the AI-generated intent and CTA buttons to appear -->
<!-- INTENT + CTA RESULTS -->
<div id="intent-box" style="display:none; margin: 2rem auto; max-width: 500px; text-align: left; background: #1a1e24; padding: 1rem; border-radius: 10px;">
  <h3 style="margin-top:0; color: #FA4B53;">üß† VoiceMate Intent Engine</h3>
  <p id="intent-summary" style="margin-bottom: 1rem; white-space: pre-wrap;">Analyzing...</p>
  <div id="cta-buttons" style="display: flex; flex-wrap: wrap; gap: 0.5rem;"></div>
</div>


<script>
  async function fetchAIIntent(transcriptText) {
    const response = await fetch("get-intent.php", {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({ transcript: transcriptText }) // transcriptText must be a string!
});


    const result = await response.json();
    if (result.success) {
      document.getElementById("intent-box").style.display = "block";
      document.getElementById("intent-summary").innerText = `üîç Detected Intent: ‚Üí "${result.intent}"`;

      const ctaContainer = document.getElementById("cta-buttons");
      ctaContainer.innerHTML = "";
      result.ctas.forEach(cta => {
        const btn = document.createElement("button");
        btn.textContent = cta;
        btn.className = "cta-button";
        ctaContainer.appendChild(btn);
      });
    }
  }

  function handleTranscriptFinal(transcriptText) {
    fetchAIIntent(transcriptText);
  }

  let mediaRecorder;
  let isRecording = false;
  let audioChunks = [];
  let startTime;
  let timerInterval;
  let recognition;
  let audioBlob;

  const orb = document.getElementById("orb");
  const status = document.getElementById("status");
  const timer = document.getElementById("timer");
  const transcript = document.getElementById("transcript");
  const audio = document.getElementById("audio");

  function formatTime(ms) {
    const totalSeconds = Math.floor(ms / 1000);
    const minutes = String(Math.floor(totalSeconds / 60)).padStart(2, '0');
    const seconds = String(totalSeconds % 60).padStart(2, '0');
    return `${minutes}:${seconds}`;
  }

  function updateTimer() {
    const elapsed = Date.now() - startTime;
    timer.textContent = formatTime(elapsed);
  }

  async function toggleRecording() {
    if (!isRecording) {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      mediaRecorder = new MediaRecorder(stream);
      audioChunks = [];

      mediaRecorder.ondataavailable = event => audioChunks.push(event.data);
      mediaRecorder.onstop = () => {
        audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
        audio.src = URL.createObjectURL(audioBlob);
        handleTranscriptFinal(transcript.value);
      };

      mediaRecorder.start();
      status.textContent = "Recording...";
      startTime = Date.now();
      timerInterval = setInterval(updateTimer, 500);
      isRecording = true;
      orb.classList.add("listening");

      recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
      recognition.continuous = true;
      recognition.interimResults = true;
      let finalTranscript = '';

      recognition.onresult = (event) => {
        let interimTranscript = '';
        for (let i = event.resultIndex; i < event.results.length; ++i) {
          const transcriptChunk = event.results[i][0].transcript;
          if (event.results[i].isFinal) {
            finalTranscript += transcriptChunk + ' ';
          } else {
            interimTranscript += transcriptChunk;
          }
        }
        transcript.value = finalTranscript + interimTranscript;
      };

      recognition.onerror = (event) => {
        status.textContent = event.error === 'no-speech' ? "No speech detected." : `Error: ${event.error}`;
      };
      recognition.start();

    } else {
      mediaRecorder.stop();
      clearInterval(timerInterval);
      recognition.stop();
      status.textContent = "Stopped";
      isRecording = false;
      orb.classList.remove("listening");
    }
  }

  orb.addEventListener("click", toggleRecording);

  function sendPulse() {
    if (!audioBlob || !transcript.value.trim()) {
      status.textContent = "Record and transcribe first.";
      return;
    }

    const formData = new FormData();
    formData.append("audio", audioBlob, "pulse.webm");
    formData.append("message", transcript.value);

    fetch("pulse-submit.php", {
      method: "POST",
      body: formData
    }).then(res => res.json())
      .then(data => {
        if (data.success) {
          status.textContent = "‚úÖ Pulse sent!";
        } else {
          status.textContent = "‚ùå Failed to send.";
        }
      })
      .catch(err => {
        console.error(err);
        status.textContent = "‚ùå Error sending pulse.";
      });
  }
</script>

</body>
</html>
